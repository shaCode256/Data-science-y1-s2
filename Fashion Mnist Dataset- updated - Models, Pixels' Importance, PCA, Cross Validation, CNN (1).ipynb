{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\97252\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\97252\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\97252\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\97252\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\97252\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\97252\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#imports for a Fashion Mnist predictor model\n",
    "\n",
    "import pandas as pd\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "pca= PCA(n_components=0.95)\n",
    "\n",
    "#reading the traiining dataset\n",
    "data=pd.read_csv(r'C:\\Users\\97252\\OneDrive\\שולחן העבודה\\הדנת- עבודת גמר\\Fashion Mnist dataset\\fashion-mnist_train.csv')\n",
    "\n",
    "X = data.drop(\"label\",axis = 1)\n",
    "y = data.label\n",
    "\n",
    "#shows the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERFORMING PCA ON THE TRAINING DATA since it's very big\n",
    "\n",
    "pcadX = pca.fit_transform(X)\n",
    "\n",
    "# splitting the data into training and testing sets, with the pcad data(features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(pcadX, y, test_size=0.2, random_state=0)\n",
    "\n",
    "pcadX.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's observe our labels!\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ... Got yourselves asking- what do those number mean?!\n",
    "# So did we!\n",
    "# Get to know the labels!\n",
    "\n",
    "1- T-shirt/top\n",
    "2- Trouser/pants\n",
    "3- Pullover shirt\n",
    "4- Dress\n",
    "5- Coat\n",
    "6- Sandal\n",
    "7- Shirt\n",
    "8- Sneaker\n",
    "9- Bag\n",
    "10- Ankle boot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph: plotting how many labels there are, in every kind- \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#matplot 1\n",
    "\n",
    "#price distribution of apps\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "axes= data.label.value_counts().plot(kind='pie')\n",
    "axes.set_title(\"Labels' Distribution\", fontsize=20)\n",
    "\n",
    "axes.set_xlabel(\"\")\n",
    "axes.set_ylabel(\"\")\n",
    "\n",
    "fig1 = plt.gcf()\n",
    "plt.show()\n",
    "plt.draw()\n",
    "\n",
    "#seems like they're equally fitted there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training and predicting on the PCA-d data using-\n",
    "#K-Neighbors\n",
    "#Decision Tree\n",
    "#Random Forest\n",
    "#AdaBoost\n",
    "#Gradient Boosting\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    RandomForestClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    ]  \n",
    "    \n",
    "for classifier in classifiers:\n",
    "    model = classifier.fit(X_train, y_train)\n",
    "    print(\"\\n The Classifier is \\n \" +str(classifier))\n",
    "    print(\"\\n The Model's Score is %.3f\" % model.score(X_test, y_test))\n",
    "    y_pred_best = model.predict(X_test)\n",
    "    cm=confusion_matrix(y_test, y_pred_best, labels=None, sample_weight=None)\n",
    "    print(\"Confusion Matrix for this\", classifier)\n",
    "    print(cm)\n",
    "    print('')\n",
    "    sns.heatmap(cfm, annot=True)\n",
    "    print('')\n",
    "    print(classification_report(y_test,y_pred_best))\n",
    "    #cross validation- only for KNN classifier (time reasons)\n",
    "    if (str(classifier) == 'KNeighborsClassifier(n_neighbors=5)'):  #CROSS VALIDATION! activate only KNN\n",
    "        score= cross_val_score(classifier, Xa_test, ya_test, cv=4)\n",
    "        print(\"This KNN model's Score with cross validation is \", score)\n",
    "        print('')\n",
    "        print(\"Accuracy by Cross Validation is: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "        print('')\n",
    "        \n",
    "        #pyplot.boxplot(score)\n",
    "        #pyplot.show()\n",
    "  \n",
    " #sns.heatmap(pd.DataFrame(classification_report).iloc[:-1, :].T, annot=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Soooo, what IS Cross Validaion?!\n",
    "It's a technique that is used for the assessment of how the results of statistical analysis \n",
    "generalize to an independent data set. Cross-validation is largely used in settings where the target is prediction\n",
    "and it is necessary to estimate the accuracy of the performance of a predictive model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOTTING PIXELS IMPORATNCE! BY RANDOM FORESTS\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "model.fit(X, y) #on the ORIGINAL dataset (features: X! so we won't lose features according to the PCA procedure)\n",
    "\n",
    "importances = model.feature_importances_\n",
    "pixels = np.reshape(importances,(28,28)) #since the shape of the dataset is (60000,728)\n",
    "\n",
    "plt.figure(figsize= (10,8))\n",
    "plot = sns.heatmap(pixels,cmap=plt.cm.hot)\n",
    "plt.xticks([], [])\n",
    "plot.set_yticks([])\n",
    "plot.set_yticks([], minor=True)\n",
    "colorbar = plot.collections[0].colorbar\n",
    "colorbar.set_ticks([0,max(importances)])\n",
    "colorbar.set_ticklabels(['not important','Very important'])\n",
    "plt.title('Pixels Importance',size = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fun fact! this is the second time Random Forest is used. the first time on the PCA-d data, and now on the ORIGINAL one.\n",
    "\n",
    "#Let's compare the model's results according to the datas!\n",
    "\n",
    "print('Random Forest classifier on the ORIGINAL data results in:')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"\\n The Model's Score is %.3f\" % model.score(X_test, y_test))\n",
    "\n",
    "y_pred_best = model.predict(X_test)\n",
    "cm=confusion_matrix(y_test, y_pred_best, labels=None, sample_weight=None)\n",
    "print(cm)\n",
    "print(classification_report(y_test,y_pred_best))\n",
    "\n",
    "#seeing this, the model works better on the PCA-d data. since it has less features.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "\n",
    "#predicting by Logistic Regression on the ORIGINAL dataset (not the PCA-d one)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#Creating and training the model\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train, y_train)\n",
    "predictions = logmodel.predict(X_test)\n",
    "\n",
    "#Evaluating the model\n",
    "cfm = metrics.confusion_matrix(y_test, predictions)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "#Naive Bayes algorithm\n",
    "\n",
    "naive_bayes = GaussianNB()\n",
    "print(np.mean(cross_val_score(estimator=naive_bayes, cv=4, scoring='accuracy', X=x_train, y=y_train)))\n",
    "naive_bayes.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN- NEURAL NETWORK predictor\n",
    "\n",
    "#Define, Compile, and Fit the Keras Classification Model\n",
    "\n",
    "#Reshape the inputs (X_train and X_test) to a shape that can be an input for the CNN model.\n",
    "#The Keras reshape function takes as arguments the number of images\n",
    "#(60,000 for X_train and 10,000 for X_test), the shape of each image (28×28), and\n",
    "#the number of color channels – 1 in this case because images are greyscale.\n",
    "\n",
    "#reading the dataset imported\n",
    "\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "# loading the Fashion-Mnist dataset\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],28,28,1)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0],28,28,1)\n",
    "\n",
    "#Then, one-hot-encode the target variable, mapping a variable to each target label – \n",
    "#in our case, ‘0’, ‘1’, ‘2’, etc. because we are recognizing the type of products as cloths and bag. \n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "y_train[0]\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# normalize to range 0-1\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "\n",
    "# build the model\n",
    "model.fit(X_train, y_train, epochs=1)\n",
    "\n",
    "print('')\n",
    "\n",
    "pred_test= model.predict(X_test)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy on test data: {}% \\n Error on test data: {}'.format(scores[1], 1 - scores[1]))   \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Some explaining about the CNN model\n",
    "\n",
    "Two “Conv2D” or 2-dimensional convolutional layers, each with a pooling layer following it.\n",
    "The first layer uses 64 nodes, while the second uses 32,\n",
    "and ‘kernel’ or filter size for both is 3 squared pixels.\n",
    "A “flatten” layer that turns the inputs into a vector\n",
    "A “dense” layer that takes that vector and generates probabilities for 10 target labels, using a Softmax activation function.\n",
    "\n",
    "Optimizer – use the ‘adam’ optimize which adjusts learning rate throughout training\n",
    "Loss function – use a ‘categorical_crossentropy’ loss function, a common choice for classification. \n",
    "The lower the score, the better the model is performing.\n",
    "Metrics – use the ‘accuracy’ metric to get an accuracy score when the model runs on the validation set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
