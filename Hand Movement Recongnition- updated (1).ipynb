{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\97252\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\97252\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\97252\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\97252\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\97252\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\97252\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\97252\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\97252\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\97252\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\97252\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\97252\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\97252\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# baseline cnn model for fashion mnist\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "#import stuff for the project\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "#sklearn stuff importing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#for data cleaning- organizing\n",
    "import glob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Frame ID</th>\n",
       "      <th>Hand Type</th>\n",
       "      <th># hands</th>\n",
       "      <th>Position X</th>\n",
       "      <th>Position Y</th>\n",
       "      <th>Position Z</th>\n",
       "      <th>Velocity X</th>\n",
       "      <th>Velocity Y</th>\n",
       "      <th>Velocity Z</th>\n",
       "      <th>...</th>\n",
       "      <th>Wrist Pos X</th>\n",
       "      <th>Wrist Pos Y</th>\n",
       "      <th>Wrist Pos Z</th>\n",
       "      <th>Elbow pos X</th>\n",
       "      <th>Elbow Pos Y</th>\n",
       "      <th>Elbow Pos Z</th>\n",
       "      <th>Grab Strenth</th>\n",
       "      <th>Grab Angle</th>\n",
       "      <th>Pinch Strength</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>335.9984</td>\n",
       "      <td>128556</td>\n",
       "      <td>right</td>\n",
       "      <td>2</td>\n",
       "      <td>128.63570</td>\n",
       "      <td>224.0294</td>\n",
       "      <td>-12.29588</td>\n",
       "      <td>-18.26243</td>\n",
       "      <td>-140.54450</td>\n",
       "      <td>-117.57790</td>\n",
       "      <td>...</td>\n",
       "      <td>177.5125</td>\n",
       "      <td>173.5658</td>\n",
       "      <td>-14.569630</td>\n",
       "      <td>276.8753</td>\n",
       "      <td>-33.68264</td>\n",
       "      <td>91.61740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.250576</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>335.9984</td>\n",
       "      <td>128556</td>\n",
       "      <td>left</td>\n",
       "      <td>2</td>\n",
       "      <td>-93.24845</td>\n",
       "      <td>143.9423</td>\n",
       "      <td>-37.81597</td>\n",
       "      <td>-109.09820</td>\n",
       "      <td>240.38980</td>\n",
       "      <td>-67.62959</td>\n",
       "      <td>...</td>\n",
       "      <td>-147.0455</td>\n",
       "      <td>109.9380</td>\n",
       "      <td>-5.326688</td>\n",
       "      <td>-329.8577</td>\n",
       "      <td>-34.41571</td>\n",
       "      <td>93.88171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>336.0150</td>\n",
       "      <td>128558</td>\n",
       "      <td>right</td>\n",
       "      <td>2</td>\n",
       "      <td>129.15550</td>\n",
       "      <td>221.9923</td>\n",
       "      <td>-15.00181</td>\n",
       "      <td>47.36464</td>\n",
       "      <td>-64.25022</td>\n",
       "      <td>-168.85850</td>\n",
       "      <td>...</td>\n",
       "      <td>177.7894</td>\n",
       "      <td>171.1952</td>\n",
       "      <td>-14.677850</td>\n",
       "      <td>276.0645</td>\n",
       "      <td>-33.15913</td>\n",
       "      <td>97.88171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.069930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>336.0150</td>\n",
       "      <td>128558</td>\n",
       "      <td>left</td>\n",
       "      <td>2</td>\n",
       "      <td>-94.86554</td>\n",
       "      <td>148.5542</td>\n",
       "      <td>-39.00158</td>\n",
       "      <td>-80.99012</td>\n",
       "      <td>278.60220</td>\n",
       "      <td>-66.79356</td>\n",
       "      <td>...</td>\n",
       "      <td>-148.2022</td>\n",
       "      <td>113.0909</td>\n",
       "      <td>-7.375025</td>\n",
       "      <td>-326.9279</td>\n",
       "      <td>-39.97873</td>\n",
       "      <td>85.98000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>336.0315</td>\n",
       "      <td>128560</td>\n",
       "      <td>right</td>\n",
       "      <td>2</td>\n",
       "      <td>129.87070</td>\n",
       "      <td>220.7944</td>\n",
       "      <td>-18.38681</td>\n",
       "      <td>35.05422</td>\n",
       "      <td>-84.74242</td>\n",
       "      <td>-205.72030</td>\n",
       "      <td>...</td>\n",
       "      <td>178.3730</td>\n",
       "      <td>169.9159</td>\n",
       "      <td>-15.246240</td>\n",
       "      <td>276.1767</td>\n",
       "      <td>-30.72506</td>\n",
       "      <td>104.13920</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.916969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time   Frame ID  Hand Type   # hands   Position X   Position Y  \\\n",
       "0  335.9984     128556      right         2    128.63570     224.0294   \n",
       "1  335.9984     128556       left         2    -93.24845     143.9423   \n",
       "2  336.0150     128558      right         2    129.15550     221.9923   \n",
       "3  336.0150     128558       left         2    -94.86554     148.5542   \n",
       "4  336.0315     128560      right         2    129.87070     220.7944   \n",
       "\n",
       "    Position Z   Velocity X   Velocity Y   Velocity Z  ...   Wrist Pos X  \\\n",
       "0    -12.29588    -18.26243   -140.54450   -117.57790  ...      177.5125   \n",
       "1    -37.81597   -109.09820    240.38980    -67.62959  ...     -147.0455   \n",
       "2    -15.00181     47.36464    -64.25022   -168.85850  ...      177.7894   \n",
       "3    -39.00158    -80.99012    278.60220    -66.79356  ...     -148.2022   \n",
       "4    -18.38681     35.05422    -84.74242   -205.72030  ...      178.3730   \n",
       "\n",
       "    Wrist Pos Y   Wrist Pos Z   Elbow pos X   Elbow Pos Y   Elbow Pos Z  \\\n",
       "0      173.5658    -14.569630      276.8753     -33.68264      91.61740   \n",
       "1      109.9380     -5.326688     -329.8577     -34.41571      93.88171   \n",
       "2      171.1952    -14.677850      276.0645     -33.15913      97.88171   \n",
       "3      113.0909     -7.375025     -326.9279     -39.97873      85.98000   \n",
       "4      169.9159    -15.246240      276.1767     -30.72506     104.13920   \n",
       "\n",
       "    Grab Strenth   Grab Angle   Pinch Strength  State  \n",
       "0            0.0     1.250576              0.0      1  \n",
       "1            0.0     0.014736              0.0      1  \n",
       "2            0.0     1.069930              0.0      1  \n",
       "3            0.0     0.000000              0.0      1  \n",
       "4            0.0     0.916969              0.0      1  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1=pd.read_csv(r'C:\\Users\\97252\\OneDrive\\שולחן העבודה\\הדנת- עבודת גמר\\Unity Data\\Training\\Evyatar Cohen\\Evyatar636771052727603804Spontan.csv')\n",
    "a1['State']= '1'\n",
    "a1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spontan- as 1. Sync- as 2. Alone-as 3.\n",
    "\n",
    "#PROCESSING THE TRAINING DATA \n",
    "\n",
    "i=1\n",
    "dfs = pd.DataFrame()\n",
    "dfs = dfs.fillna(0)\n",
    "\n",
    "for path in glob.glob(r'C:\\Users\\97252\\OneDrive\\שולחן העבודה\\הדנת- עבודת גמר\\Unity Data\\Training\\*'): \n",
    "    paths= path+\"\\*\"\n",
    "    for pathy in glob.glob(paths): \n",
    "        name=str(i)\n",
    "        name=pd.read_csv(pathy)\n",
    "        if \"Spontan\" in pathy:\n",
    "            name['State']= '1'\n",
    "        if \"Sync\" in pathy:\n",
    "           name['State']= '2'\n",
    "        if \"Alone\" in pathy:\n",
    "           name['State']= '3'\n",
    "        i += 1\n",
    "        dfs = pd.concat([dfs, name])\n",
    "        \n",
    "#dfs.head() \n",
    "dfs.to_csv(r'C:\\Users\\97252\\OneDrive\\שולחן העבודה\\הדנת- עבודת גמר\\dfs.csv', index = False) #checking if this is actually the dataset we wish to gey\n",
    "\n",
    "dfs.columns = dfs.columns.str.strip() #dealing with error, caused by leading whistespaces in column names in the csv file\n",
    "\n",
    "gp=dfs.groupby('Hand Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Right\n",
    "righty= gp.get_group('right')\n",
    "righty.columns = [str(col) + '_Right' for col in righty.columns]\n",
    "righty\n",
    "\n",
    "#Left\n",
    "lefty= gp.get_group('left')\n",
    "lefty.columns = [str(col) + '_Left' for col in lefty.columns]\n",
    "lefty\n",
    "\n",
    "#reset_indexes\n",
    "righty=righty.reset_index(drop=True)\n",
    "lefty=lefty.reset_index(drop=True)\n",
    "\n",
    "#concat them\n",
    "done=pd.concat([righty, lefty], axis=1)\n",
    "done.to_csv(r'C:\\Users\\97252\\OneDrive\\שולחן העבודה\\הדנת- עבודת גמר\\training set.csv', index = False) #checking if this is actually the dataset we wish to gey\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#editing the df\n",
    "#removing unessucery columns, which will ruin the prediction process\n",
    "\n",
    "done=done.drop('Frame ID_Right', axis=1)\n",
    "done=done.drop(labels='Frame ID_Left', axis=1)\n",
    "done=done.drop(labels='Hand Type_Right', axis=1) \n",
    "done=done.drop(labels='Hand Type_Left', axis=1)\n",
    "done=done.drop(labels='# hands_Left', axis=1) \n",
    "done=done.drop(labels='# hands_Right', axis=1) \n",
    "done=done.drop(labels='Time_Left', axis=1) \n",
    "done=done.drop(labels='Time_Right', axis=1) \n",
    "done=done.drop(labels='State_Right', axis=1) \n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill NaNs with 0 so we can build a model\n",
    "\n",
    "done=done.fillna(0)\n",
    "\n",
    "#change the df from str to int to apply numeric calculations needed for the model\n",
    "\n",
    "done=done.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "#done.columns = done.columns.str.strip().str.replace(' ', '').str.replace('(', '').str.replace(')', '')\n",
    "\n",
    "done.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph: Training data's labels' distribution!\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(font_scale=1.3)  # crazy big\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "axes = sns.countplot(y=done.State_Left)\n",
    "\n",
    "csfont = {'fontname':'Comic Sans MS'}\n",
    "\n",
    "axes.set_title(\"Training data's labels' distribution!\",**csfont,fontsize=25)\n",
    "axes.set_ylabel('Spontan- 1. Sync- 2. Alone- 3.',fontsize=20,**csfont)\n",
    "axes.set_xlabel(' Number of occurances ',fontsize=20,**csfont)\n",
    "\n",
    "fig1 = plt.gcf()\n",
    "plt.show()\n",
    "plt.draw()\n",
    "\n",
    "#Seems like they're equally divided there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tTEST dataset preparing\n",
    "\n",
    "#gathering the dataset  from the Validation folder\n",
    "\n",
    "#Spontan- as 1. Sync- as 2. Alone-as 3.\n",
    "\n",
    "import pandas as pd\n",
    "import glob #for collecting the datasets\n",
    "\n",
    "i=1\n",
    "dfsa = pd.DataFrame()\n",
    "dfsa = dfsa.fillna(0)\n",
    "\n",
    "for path in glob.glob(r'C:\\Users\\97252\\OneDrive\\שולחן העבודה\\הדנת- עבודת גמר\\Unity Data\\Validation\\*'): \n",
    "    paths= path+\"\\*\"\n",
    "    for pathy in glob.glob(paths): \n",
    "        name=str(i)\n",
    "        name=pd.read_csv(pathy)\n",
    "        if \"Spontan\" in pathy:\n",
    "            name['State']= '1'\n",
    "        if \"Sync\" in pathy:\n",
    "           name['State']= '2'\n",
    "        if \"Alone\" in pathy:\n",
    "           name['State']= '3'\n",
    "        i += 1\n",
    "        dfsa = pd.concat([dfsa, name])\n",
    "        \n",
    "#dfs.head() \n",
    "dfsa.to_csv(r'C:\\Users\\97252\\OneDrive\\שולחן העבודה\\הדנת- עבודת גמר\\dfs.csv', index = False) #checking if this is actually the dataset we wish to gey\n",
    "\n",
    "dfsa.columns = dfsa.columns.str.strip() #dealing with error, caused by leading whistespaces in column names in the csv file\n",
    "\n",
    "\n",
    "gpa=dfsa.groupby('Hand Type')\n",
    "\n",
    "\n",
    "#Hand Type: Right\n",
    "rightya= gpa.get_group('right')\n",
    "rightya.columns = [str(col) + '_Right' for col in rightya.columns]\n",
    "rightya\n",
    "\n",
    "#Hand Type: Left\n",
    "leftya= gpa.get_group('left')\n",
    "leftya.columns = [str(col) + '_Left' for col in leftya.columns]\n",
    "leftya\n",
    "\n",
    "#reset_indexes\n",
    "rightya=rightya.reset_index(drop=True)\n",
    "leftya=leftya.reset_index(drop=True)\n",
    "\n",
    "#concat them\n",
    "testy=pd.concat([rightya, leftya], axis=1)\n",
    "testy.to_csv(r'C:\\Users\\97252\\OneDrive\\שולחן העבודה\\הדנת- עבודת גמר\\test set.csv', index = False) #checking if this is actually the dataset we wish to gey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#editing the df\n",
    "#removing unessucery columns, which will ruin the prediction process\n",
    "\n",
    "testy=testy.drop('Frame ID_Right', axis=1)\n",
    "testy=testy.drop(labels='Frame ID_Left', axis=1)\n",
    "testy=testy.drop(labels='Hand Type_Right', axis=1) \n",
    "testy=testy.drop(labels='Hand Type_Left', axis=1)\n",
    "testy=testy.drop(labels='# hands_Left', axis=1) \n",
    "testy=testy.drop(labels='# hands_Right', axis=1) \n",
    "testy=testy.drop(labels='Time_Left', axis=1) \n",
    "testy=testy.drop(labels='Time_Right', axis=1) \n",
    "testy=testy.drop(labels='State_Right', axis=1) \n",
    "\n",
    "testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph: Testing data's labels' distribution!\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(font_scale=1.3)  # crazy big\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "axes = sns.countplot(y=testy.State_Left)\n",
    "\n",
    "csfont = {'fontname':'Comic Sans MS'}\n",
    "\n",
    "axes.set_title(\"Testing data's labels' distribution!\",**csfont,fontsize=25)\n",
    "axes.set_ylabel('Spontan- 1. Sync- 2. Alone- 3.',fontsize=20,**csfont)\n",
    "axes.set_xlabel(' Number of occurances ',fontsize=20,**csfont)\n",
    "\n",
    "fig1 = plt.gcf()\n",
    "plt.show()\n",
    "plt.draw()\n",
    "\n",
    "#Seems like they're equally divided there too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph: Showing correlation between the columns in the training data!\n",
    "\n",
    "#Pandas dataframe.corr() is used to find the pairwise correlation of all columns in the dataframe.\n",
    "#Any na values are automatically excluded. \n",
    "#For any non-numeric data type columns in the dataframe it is ignored\n",
    "#Note: The correlation of a variable with itself is 1.\n",
    "\n",
    "#Using corr() function to find the correlation among\n",
    "#the columns in the dataframe using ‘kendall’ method:\n",
    "\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "axes=sns.heatmap(done.corr(method ='kendall'),annot=True,square=True,cmap=\"gist_rainbow_r\", linewidths=.5)\n",
    "axes.set_title(\"Correlation between The Coloumns: Training data!\",**csfont,fontsize=50)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's fill NaNs with 0 so we can build a model\n",
    "\n",
    "testy=testy.fillna(0)\n",
    "\n",
    "#change the df from str to int to apply numeric calculations needed for the model\n",
    "\n",
    "testy=testy.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "testy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the cnn stuff we can put back if needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the Data and Performing Basic Data Checks\n",
    "\n",
    "df=done #this is the training set of this project\n",
    "dfa=testy #this is the testing set of this project\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Arrays for the Features and the Response Variable.\n",
    "\n",
    "target_column = ['State_Left'] \n",
    "predictors = list(set(list(df.columns))-set(target_column))\n",
    "#print(predictors)\n",
    "df[predictors] = df[predictors]/df[predictors].max()\n",
    "#print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Training and Test Datasets\n",
    "\n",
    "#train\n",
    "\n",
    "X_train = df[predictors].values\n",
    "y_train = df[target_column].values\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=40)\n",
    "\n",
    "#test\n",
    "Xa_test = dfa[predictors].values\n",
    "ya_test = dfa[target_column].values\n",
    "\n",
    "#Xa_train, Xa_test, ya_train, ya_test = train_test_split(Xa, ya, test_size=0.30, random_state=40)\n",
    "\n",
    "print('X_train.shape is ',X_train.shape); print('Xa_test.shape is ',Xa_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "y_train=y_train.reshape(-1, 1) #make it 1d array for the function\n",
    "# Split the data (X, Y)\n",
    "from sklearn import metrics\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "X_train = df[predictors].values\n",
    "y_train = df[target_column].values\n",
    "\n",
    "Xa_test = dfa[predictors].values #it doesn't get one-hot encoded, so we take the real values as they were at the beginning\n",
    "ya_test = dfa[target_column].values\n",
    "\n",
    "#Creating and training the model\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train, y_train)\n",
    "predictions = logmodel.predict(Xa_test)\n",
    "print(\"\\n Logistic Regression- Score is %.3f\" % logmodel.score(Xa_test, ya_test))\n",
    "\n",
    "#Evaluating the model\n",
    "cfm = metrics.confusion_matrix(ya_test, predictions)\n",
    "print(cfm)\n",
    "print('')\n",
    "#    sns.heatmap(cfm, annot=True)\n",
    "print(classification_report(ya_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTION BY ALGORITHMS:\n",
    "# KNeighbors\n",
    "#DecisionTree\n",
    "#RandomForest\n",
    "#Adaboost\n",
    "#Gradient Boosting\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(10),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier()\n",
    "    ]\n",
    "for classifier in classifiers:\n",
    "    model = classifier.fit(X_train, y_train)\n",
    "    print(\"\\n The Classifier is \\n \" +str(classifier))\n",
    "    print(\"\\n The Model's Score is %.3f\" % model.score(Xa_test, ya_test))\n",
    "    y_pred_best = model.predict(Xa_test)\n",
    "    \n",
    "    cm=confusion_matrix(ya_test, y_pred_best, labels=None, sample_weight=None)\n",
    "    print(cm)\n",
    "    print('')\n",
    "    \n",
    "    #cross validation\n",
    "    print(classification_report(ya_test,y_pred_best))\n",
    "    if (str(classifier) == 'KNeighborsClassifier(n_neighbors=10)'):  #CROSS VALIDATION! activate only KNN\n",
    "        score= cross_val_score(classifier, X_train, y_train, cv=4)\n",
    "        print(\"the Cross Validation score of this model is \", score)\n",
    "        \n",
    "    #sns.heatmap(pd.DataFrame(classification_report).iloc[:-1, :].T, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nuerual Network classification model \n",
    "#Goal: predicting whether the person is talking in a sync, alone or spontan mode\n",
    "\n",
    "#Loading the Required Libraries and Modules\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Keras specific\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREAPRING THE DATA FOR THE CNN MODEL!\n",
    "\n",
    "#Since our target variable represents a binary category which has been coded as numbers 0 and 1,\n",
    "#we will have to encode it. We can easily achieve that using the \"to_categorical\" function\n",
    "#from the Keras utilities package.\n",
    "\n",
    "#The two lines of code below accomplishes that in both training and test datasets.\n",
    "# one hot encoded outputs:\n",
    "\n",
    "import numpy\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "ya_test = to_categorical(ya_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Fun fact! What is One Hot Encoding?\n",
    "A one hot encoding is a representation of categorical variables as binary vectors.\n",
    "\n",
    "This first requires that the categorical values be mapped to integer values.\n",
    "\n",
    "Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define, Compile, and Fit the Keras Classification Model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(500, activation='relu', input_dim=36))  #the input layer which specifies the activation function and the number of input dimensions, which in our case is 36 predictors.\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax')) #The fifth line of code creates the output layer with four nodes because there are four output classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# normalize to range 0-1\n",
    "X_train = X_train / 255.0\n",
    "Xa_test = Xa_test / 255.0\n",
    "\n",
    "\n",
    "# build the model\n",
    "model.fit(X_train, y_train, epochs=3)\n",
    "\n",
    "#Predict on the Test Data and Compute Evaluation Metrics;\n",
    "\n",
    "#pred_train= model.predict(X_train)\n",
    "#scores2 = model.evaluate(X_train, y_train, verbose=0)\n",
    "#print('Accuracy on train data: {}% \\n Error on train data: {}'.format(scores2[1], 1 - scores2[1]))    \n",
    "\n",
    "print('')\n",
    "\n",
    "pred_test= model.predict(Xa_test)\n",
    "scores = model.evaluate(Xa_test, ya_test, verbose=0)\n",
    "print('Accuracy on test data: {}% \\n Error on test data: {}'.format(scores[1], 1 - scores[1]))   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get precision, recall and F-SCORE\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(X_train, y_train, validation_split=0.3, epochs=10, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(Xa_test, ya_test, verbose=0)\n",
    "\n",
    "#cfm = metrics.confusion_matrix(ya_test, predictions)\n",
    "#sns.heatmap(cfm, annot=True)\n",
    "\n",
    "print(\"The model's F-SCORE on test set is \",f1_score)\n",
    "print('')\n",
    "print(\"The model's precision on test set is \",precision)\n",
    "print('')\n",
    "print(\"The model's recall on test set is \",recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
